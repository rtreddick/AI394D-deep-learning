{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/timreddick/code/AI394D-deep-learning/deeplearning/homework_04/drive_data/val\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from deeplearning.homework_04.grader.datasets import road_dataset\n",
    "\n",
    "path = Path().cwd().parent / \"homework_04\" / \"drive_data\" / \"val\"\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 samples from 4 episodes\n"
     ]
    }
   ],
   "source": [
    "data = road_dataset.load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n",
      "Reshaped Tensor (3x4):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Reshaped Tensor (2x2x3):\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "\n",
      "Original Tensor After Reshaping:\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n",
      "Reshaped Tensor with -1 (3x-1):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 1D tensor with 12 elements\n",
    "original_tensor = torch.arange(12)\n",
    "print(\"Original Tensor:\")\n",
    "print(original_tensor)\n",
    "\n",
    "# Reshape the tensor into a 2D tensor of shape (3, 4)\n",
    "reshaped_tensor = original_tensor.reshape(3, 4)\n",
    "print(\"\\nReshaped Tensor (3x4):\")\n",
    "print(reshaped_tensor)\n",
    "\n",
    "# Reshape the tensor into a 3D tensor of shape (2, 2, 3)\n",
    "reshaped_tensor_3d = original_tensor.reshape(2, 2, 3)\n",
    "print(\"\\nReshaped Tensor (2x2x3):\")\n",
    "print(reshaped_tensor_3d)\n",
    "\n",
    "# Demonstrating that the original tensor remains unchanged\n",
    "print(\"\\nOriginal Tensor After Reshaping:\")\n",
    "print(original_tensor)\n",
    "\n",
    "# What does -1 mean in reshape?\n",
    "# The -1 in reshape allows PyTorch to automatically calculate the size of that dimension based on the other dimensions and the total number of elements.\n",
    "# For example, if we want to reshape the original tensor into a shape of (3, -1), PyTorch will automatically determine the second dimension size.\n",
    "reshaped_tensor_auto = original_tensor.reshape(3, -1)\n",
    "print(\"\\nReshaped Tensor with -1 (3x-1):\")\n",
    "print(reshaped_tensor_auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Hidden Layer 1 (h1): 0.59326999\n",
      "Output of Hidden Layer 2 (h2): 0.59688438\n",
      "Output of Output Layer 1 (o1): 0.75136507\n",
      "Output of Output Layer 2 (o2): 0.77292847\n",
      "\n",
      "Loss for Output Layer 1: 0.27481108\n",
      "Loss for Output Layer 2: 0.02356003\n",
      "Total Loss: 0.29837111\n"
     ]
    }
   ],
   "source": [
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# Hidden layer 1 calculation\n",
    "net_h1 = 0.15 * 0.05 + 0.20 * 0.10 + 0.35\n",
    "out_h1 = 1 / (1 + math.exp(-net_h1))\n",
    "print(f\"Output of Hidden Layer 1 (h1): {out_h1:.8f}\")\n",
    "\n",
    "# Hidden layer 2 calculation\n",
    "net_h2 = 0.25 * 0.05 + 0.30 * 0.10 + 0.35\n",
    "out_h2 = 1 / (1 + math.exp(-net_h2))\n",
    "print(f\"Output of Hidden Layer 2 (h2): {out_h2:.8f}\")\n",
    "\n",
    "# Output layer 1 calculation\n",
    "net_o1 = 0.40 * out_h1 + 0.45 * out_h2 + 0.60\n",
    "out_o1 = 1 / (1 + math.exp(-net_o1))\n",
    "print(f\"Output of Output Layer 1 (o1): {out_o1:.8f}\")\n",
    "\n",
    "# Output layer 2 calculation\n",
    "net_o2 = 0.50 * out_h1 + 0.55 * out_h2 + 0.60\n",
    "out_o2 = 1 / (1 + math.exp(-net_o2))\n",
    "print(f\"Output of Output Layer 2 (o2): {out_o2:.8f}\")\n",
    "\n",
    "# Calculate loss\n",
    "target_o1 = 0.01\n",
    "target_o2 = 0.99\n",
    "loss_1 = 1/2 * ((target_o1 - out_o1) ** 2)\n",
    "loss_2 = 1/2 * ((target_o2 - out_o2) ** 2)\n",
    "loss = loss_1 + loss_2\n",
    "print(f\"\\nLoss for Output Layer 1: {loss_1:.8f}\")\n",
    "print(f\"Loss for Output Layer 2: {loss_2:.8f}\")\n",
    "print(f\"Total Loss: {loss:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Automatic Gradients Example\n",
    "\n",
    "This example demonstrates how PyTorch automatically calculates gradients without needing to explicitly define the gradient form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂E_total/∂w5 = 0.082329690\n",
      "\n",
      "Compare to manually calculated value in the article: 0.082167041\n",
      "\n",
      "Gradients for other parameters:\n",
      "∂E_total/∂w1 = 0.000438205\n",
      "∂E_total/∂w2 = 0.000498006\n",
      "∂E_total/∂b1 = 0.018724229\n",
      "∂E_total/∂b2 = 0.100407004\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensors with requires_grad=True to track computations\n",
    "w1 = torch.tensor(0.15, requires_grad=True)\n",
    "w2 = torch.tensor(0.20, requires_grad=True)\n",
    "w3 = torch.tensor(0.25, requires_grad=True)\n",
    "w4 = torch.tensor(0.30, requires_grad=True)\n",
    "w5 = torch.tensor(0.40, requires_grad=True)  # The weight we were analyzing\n",
    "w6 = torch.tensor(0.45, requires_grad=True)\n",
    "w7 = torch.tensor(0.50, requires_grad=True)\n",
    "w8 = torch.tensor(0.55, requires_grad=True)\n",
    "\n",
    "b1 = torch.tensor(0.35, requires_grad=True)\n",
    "b2 = torch.tensor(0.60, requires_grad=True)\n",
    "\n",
    "# Input values\n",
    "i1 = torch.tensor(0.05)\n",
    "i2 = torch.tensor(0.10)\n",
    "\n",
    "# Forward pass - same as our manual calculations but using PyTorch\n",
    "# Hidden layer\n",
    "net_h1 = w1 * i1 + w3 * i2 + b1\n",
    "out_h1 = 1 / (1 + torch.exp(-net_h1))\n",
    "\n",
    "net_h2 = w2 * i1 + w4 * i2 + b1\n",
    "out_h2 = 1 / (1 + torch.exp(-net_h2))\n",
    "\n",
    "# Output layer\n",
    "net_o1 = w5 * out_h1 + w6 * out_h2 + b2\n",
    "out_o1 = 1 / (1 + torch.exp(-net_o1))\n",
    "\n",
    "net_o2 = w7 * out_h1 + w8 * out_h2 + b2\n",
    "out_o2 = 1 / (1 + torch.exp(-net_o2))\n",
    "\n",
    "# Target values\n",
    "t1 = torch.tensor(0.01)\n",
    "t2 = torch.tensor(0.99)\n",
    "\n",
    "# Calculate the error (loss)\n",
    "error_o1 = 0.5 * (t1 - out_o1) ** 2\n",
    "error_o2 = 0.5 * (t2 - out_o2) ** 2\n",
    "error_total = error_o1 + error_o2\n",
    "\n",
    "# Backpropagation - PyTorch calculates all gradients automatically!\n",
    "error_total.backward()\n",
    "\n",
    "# Print the gradient of the total error with respect to w5\n",
    "print(f\"∂E_total/∂w5 = {w5.grad:.9f}\")\n",
    "print(f\"\\nCompare to manually calculated value in the article: 0.082167041\")\n",
    "\n",
    "# You can also check gradients for other parameters\n",
    "print(f\"\\nGradients for other parameters:\")\n",
    "print(f\"∂E_total/∂w1 = {w1.grad:.9f}\")\n",
    "print(f\"∂E_total/∂w2 = {w2.grad:.9f}\")\n",
    "print(f\"∂E_total/∂b1 = {b1.grad:.9f}\")\n",
    "print(f\"∂E_total/∂b2 = {b2.grad:.9f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Automatic Differentiation Example\n",
    "\n",
    "This demonstrates how PyTorch calculates gradients behind the scenes without knowing the form of the gradient beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"Initial x value: {x.item()}\")\n",
    "\n",
    "# Step 1: Perform forward computation\n",
    "# Let's say we compute y = x^2\n",
    "y = x * x\n",
    "print(f\"y = x^2 = {y.item()}\")\n",
    "\n",
    "# Step 2: Call backward() to compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Step 3: Access the computed gradient\n",
    "print(f\"Gradient of y with respect to x (dy/dx): {x.grad.item()}\")\n",
    "print(f\"Expected value: 2x = 2*2 = 4\")\n",
    "print(\"\\nPyTorch calculated the correct gradient dy/dx = 4 without us specifying the formula dy/dx = 2x\")\n",
    "\n",
    "# Let's see a more complex example\n",
    "print(\"\\n----- More Complex Example -----\")\n",
    "a = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Complex computation: z = (a^2 + b)^3\n",
    "c = a * a + b\n",
    "z = c * c * c\n",
    "print(f\"z = (a^2 + b)^3 = {z.item()}\")\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(f\"dz/da = {a.grad.item()}\")\n",
    "print(f\"dz/db = {b.grad.item()}\")\n",
    "print(\"\\nPyTorch correctly calculated these gradients through chain rule without us explicitly providing the formulas:\")\n",
    "print(\"dz/da = 3(a^2 + b)^2 * 2a\")\n",
    "print(\"dz/db = 3(a^2 + b)^2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Reuse in Backpropagation\n",
    "\n",
    "This demonstrates why we can reuse gradient calculations in backpropagation, as mentioned in the article. When computing the gradient for weights between input and hidden layers, we reuse components calculated for the output layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculations to demonstrate gradient reuse in backpropagation\n",
    "import math\n",
    "\n",
    "# Forward pass calculations (same as before)\n",
    "# Input values\n",
    "i1, i2 = 0.05, 0.10\n",
    "\n",
    "# Weights\n",
    "w1, w2 = 0.15, 0.20  # w1, w2 connect i1, i2 to h1\n",
    "w3, w4 = 0.25, 0.30  # w3, w4 connect i1, i2 to h2\n",
    "w5, w6 = 0.40, 0.45  # w5, w6 connect h1, h2 to o1\n",
    "w7, w8 = 0.50, 0.55  # w7, w8 connect h1, h2 to o2\n",
    "\n",
    "# Biases\n",
    "b1 = 0.35  # hidden layer bias\n",
    "b2 = 0.60  # output layer bias\n",
    "\n",
    "# Target values\n",
    "t1, t2 = 0.01, 0.99\n",
    "\n",
    "# Hidden layer outputs\n",
    "net_h1 = w1 * i1 + w3 * i2 + b1\n",
    "out_h1 = 1 / (1 + math.exp(-net_h1))\n",
    "\n",
    "net_h2 = w2 * i1 + w4 * i2 + b1\n",
    "out_h2 = 1 / (1 + math.exp(-net_h2))\n",
    "\n",
    "# Output layer outputs\n",
    "net_o1 = w5 * out_h1 + w6 * out_h2 + b2\n",
    "out_o1 = 1 / (1 + math.exp(-net_o1))\n",
    "\n",
    "net_o2 = w7 * out_h1 + w8 * out_h2 + b2\n",
    "out_o2 = 1 / (1 + math.exp(-net_o2))\n",
    "\n",
    "# Calculate error\n",
    "E_o1 = 0.5 * (t1 - out_o1) ** 2\n",
    "E_o2 = 0.5 * (t2 - out_o2) ** 2\n",
    "E_total = E_o1 + E_o2\n",
    "\n",
    "print(f\"out_o1: {out_o1:.8f}, out_o2: {out_o2:.8f}\")\n",
    "print(f\"E_total: {E_total:.8f}\\n\")\n",
    "\n",
    "# Backpropagation\n",
    "# First, calculate gradients for output layer weights\n",
    "\n",
    "# For w5 (weight connecting h1 to o1)\n",
    "# Step 1: Calculate ∂E_o1/∂out_o1\n",
    "dE_o1_dout_o1 = -(t1 - out_o1)\n",
    "print(f\"∂E_o1/∂out_o1 = {dE_o1_dout_o1:.8f}\")\n",
    "\n",
    "# Step 2: Calculate ∂out_o1/∂net_o1\n",
    "dout_o1_dnet_o1 = out_o1 * (1 - out_o1)\n",
    "print(f\"∂out_o1/∂net_o1 = {dout_o1_dnet_o1:.8f}\")\n",
    "\n",
    "# Step 3: Calculate ∂net_o1/∂w5\n",
    "dnet_o1_dw5 = out_h1\n",
    "print(f\"∂net_o1/∂w5 = {dnet_o1_dw5:.8f}\")\n",
    "\n",
    "# Step 4: Calculate ∂E_o1/∂w5 (chain rule)\n",
    "dE_o1_dw5 = dE_o1_dout_o1 * dout_o1_dnet_o1 * dnet_o1_dw5\n",
    "print(f\"∂E_o1/∂w5 = {dE_o1_dw5:.8f}\\n\")\n",
    "\n",
    "# IMPORTANT PART - Reusing calculations for w1\n",
    "# To calculate ∂E_o1/∂w1, we need first ∂E_o1/∂out_h1\n",
    "\n",
    "# Step 1: We already calculated ∂E_o1/∂out_o1 and ∂out_o1/∂net_o1\n",
    "dE_o1_dnet_o1 = dE_o1_dout_o1 * dout_o1_dnet_o1\n",
    "print(\"This is where we REUSE the calculation!\")\n",
    "print(f\"∂E_o1/∂net_o1 = ∂E_o1/∂out_o1 * ∂out_o1/∂net_o1 = {dE_o1_dnet_o1:.8f}\")\n",
    "\n",
    "# Step 2: Calculate ∂net_o1/∂out_h1\n",
    "dnet_o1_dout_h1 = w5\n",
    "print(f\"∂net_o1/∂out_h1 = {dnet_o1_dout_h1:.8f}\")\n",
    "\n",
    "# Step 3: Calculate ∂E_o1/∂out_h1 (chain rule)\n",
    "dE_o1_dout_h1 = dE_o1_dnet_o1 * dnet_o1_dout_h1\n",
    "print(f\"∂E_o1/∂out_h1 = {dE_o1_dout_h1:.8f}\\n\")\n",
    "\n",
    "print(\"The calculated value matches what's in the article (0.138498562)\")\n",
    "print(\"This demonstrates how gradient calculations are reused during backpropagation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Representation of Backpropagation Flow\n",
    "\n",
    "Backpropagation flows from output to input, with gradient calculations being reused:\n",
    "\n",
    "```\n",
    "Input Layer    Hidden Layer(s)    Output Layer\n",
    "   [i1]             [h1]             [o1]\n",
    "     |               /|\\              / \\\n",
    "     |              / | \\            /   \\\n",
    "    w1,w2         w5,w7 |          /     \\\n",
    "     |            /   \\ |         /       \\\n",
    "     |           /     \\|        /         \\\n",
    "   [i2]------->[h2]     -----> [o2]       Error\n",
    "                                          |\n",
    "                                          |\n",
    "           Backpropagation Flow          \\|/\n",
    "           <---------------------------- Start here\n",
    "```\n",
    "\n",
    "Backpropagation process:\n",
    "\n",
    "1. Calculate output layer errors (o1, o2)\n",
    "2. Update weights connecting hidden to output (w5, w6, w7, w8)\n",
    "   - Calculate ∂E/∂w5, ∂E/∂w6, ∂E/∂w7, ∂E/∂w8\n",
    "3. **Reuse components** to calculate hidden layer gradients\n",
    "   - Reuse ∂E/∂net_o1 when calculating ∂E/∂out_h1\n",
    "4. Update weights connecting input to hidden (w1, w2, w3, w4)\n",
    "\n",
    "This reuse of calculations makes backpropagation efficient, especially for deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified backpropagation process visualization\n",
    "print(\"Backpropagation Process: Output → Hidden → Input\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Forward pass (input → hidden → output)\")\n",
    "print(\"2. Calculate error at output layer\")\n",
    "print(\"3. Update output layer weights (w5, w6, w7, w8)\")\n",
    "print(\"   - Calculate: ∂E/∂w_output = ∂E/∂out_o * ∂out_o/∂net_o * ∂net_o/∂w_output\")\n",
    "print(\"4. REUSE calculation ∂E/∂net_o for hidden layer\")\n",
    "print(\"   - Hidden gradients: ∂E/∂out_h = ∂E/∂net_o * ∂net_o/∂out_h\")\n",
    "print(\"5. Update hidden layer weights (w1, w2, w3, w4)\")\n",
    "print(\"   - ∂E/∂w_hidden = ∂E/∂out_h * ∂out_h/∂net_h * ∂net_h/∂w_hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
